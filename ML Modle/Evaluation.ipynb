{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b290ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from flask import request, jsonify\n",
    "from pytesseract import pytesseract\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe88c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the T5 model\n",
    "model_path = r'D:\\Personal\\Edu\\OCR-Expresso\\t5_gec_model' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94fd52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained T5 model and tokenizer\n",
    "loaded_model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path, legacy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1459da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_corrector(input_texts, num_return_sequences):\n",
    "    # Tokenize the input texts\n",
    "    batch = tokenizer(input_texts, truncation=True, padding='max_length', max_length=64, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate corrected text using the loaded model\n",
    "    translated = loaded_model.generate(**batch, max_length=64, num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5, do_sample=True)\n",
    "    \n",
    "    # Decode the generated text\n",
    "    tgt_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    \n",
    "    return tgt_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87c40fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550000, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/Personal/Edu/Study/FYP/NLP/data_set/c4_200m_550k.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a48e7fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((495000, 2), (55000, 2))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1, shuffle=False)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9dd83a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, batch_size=16):\n",
    "    # Initialize lists to store predictions and ground truth labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for start_idx in range(0, len(dataset), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(dataset))\n",
    "        batch_examples = dataset[start_idx:end_idx]\n",
    "        \n",
    "        input_texts = [example['input'] for example in batch_examples]\n",
    "        ground_truths = [example['output'] for example in batch_examples]\n",
    "\n",
    "        # Generate predictions using the model\n",
    "        predictions = grammar_corrector(input_texts, num_return_sequences=1)\n",
    "        \n",
    "        # Append predictions and ground truth to the respective lists\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(ground_truths)\n",
    "        \n",
    "        print(f\"Processed batch {start_idx // batch_size + 1}/{(len(dataset) + batch_size - 1) // batch_size}\")\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Compute precision\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    \n",
    "    # Compute recall\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    \n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=1)\n",
    "    \n",
    "    return cm , accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b40a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix, accuracy, precision, recall, f1 = evaluate_model(loaded_model, tokenizer, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c5e7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ce19895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.83636\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f85b3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.68452\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c09372bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.56147\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf9e062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
